<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ITSC 2025 Workshop: Multimodal Fusion enabled Embodied Intelligence of Autonomous Driving</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Georgia:wght@700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f0f0;
        }
        /* Style for the section headers, mimicking a formal document */
        .section-title {
            font-family: 'Georgia', serif;
            color: #8B4513; /* A brown/maroon color */
            font-weight: 700;
            border-bottom: 2px solid #ddd;
            padding-bottom: 0.5rem;
        }
        /* Style for the active nav link */
        .nav-link-active {
            color: #ffffff;
            font-weight: 600;
            border-bottom: 2px solid #3b82f6;
        }
        .hero-section {
            background-image: linear-gradient(rgba(0, 0, 0, 0.6), rgba(0, 0, 0, 0.6)), url('https://placehold.co/1920x600/334155/ffffff?text=Autonomous+Driving+Research');
            background-size: cover;
            background-position: center;
        }
        .speaker-card img, .organizer-card img {
            border: 3px solid #e5e7eb;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <nav class="bg-gray-800 text-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6 py-3">
            <div class="flex justify-between items-center">
                <a href="#home" class="text-xl font-bold">ITSC 2025 Workshop</a>
                <div class="hidden md:flex space-x-8">
                    <a href="#home" class="nav-link text-gray-300 hover:text-white transition-colors pb-1">Home</a>
                    <a href="#abstract" class="nav-link text-gray-300 hover:text-white transition-colors pb-1">Abstract</a>
                    <a href="#schedule" class="nav-link text-gray-300 hover:text-white transition-colors pb-1">Schedule</a>
                    <a href="#speakers" class="nav-link text-gray-300 hover:text-white transition-colors pb-1">Speakers</a>
                    <a href="#organizers" class="nav-link text-gray-300 hover:text-white transition-colors pb-1">Organizers</a>
                </div>
                <button id="mobile-menu-button" class="md:hidden text-gray-300 focus:outline-none">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
                </button>
            </div>
            <div id="mobile-menu" class="hidden md:hidden mt-4">
                <a href="#home" class="block py-2 px-4 text-sm text-gray-300 hover:bg-gray-700 rounded">Home</a>
                <a href="#abstract" class="block py-2 px-4 text-sm text-gray-300 hover:bg-gray-700 rounded">Abstract</a>
                <a href="#schedule" class="block py-2 px-4 text-sm text-gray-300 hover:bg-gray-700 rounded">Schedule</a>
                <a href="#speakers" class="block py-2 px-4 text-sm text-gray-300 hover:bg-gray-700 rounded">Speakers</a>
                <a href="#organizers" class="block py-2 px-4 text-sm text-gray-300 hover:bg-gray-700 rounded">Organizers</a>
            </div>
        </div>
    </nav>

    <header id="home" class="hero-section text-white">
        <div class="container mx-auto px-6 py-24 md:py-32 text-center">
            <h1 class="text-3xl md:text-5xl font-bold mb-4 leading-tight">
                Workshop on Multimodal Fusion enabled Embodied Intelligence of Autonomous Driving 
            </h1>
            <p class="text-xl text-gray-300 mt-4">ITSC 2025 |  November 18, 2025 </p>
        </div>
    </header>

    <main class="container mx-auto p-4 sm:p-6 md:p-8 bg-white -mt-16 rounded-lg shadow-lg z-10 relative">

        <section id="abstract" class="my-12 scroll-mt-20">
            <h2 class="text-3xl section-title mb-6">Abstract</h2>
            <div class="max-w-4xl mx-auto text-gray-700 space-y-4 text-left">
                 <p>The convergence of multimodal fusion and embodied intelligence has emerged as a pivotal frontier in advancing autonomous driving systems.  Modern autonomous vehicles rely on integrating heterogeneous data streams, such as visual, LiDAR, tactile, and auditory inputs, to perceive dynamic environments and make human-like decisions.   However, existing approaches often face challenges in achieving robust cross-modal alignment, real-time adaptability, and contextual reasoning under complex scenarios (e.g., occlusions, unpredictable human behavior, or adverse weather).   For instance, traditional bird's-eye-view (BEV) methods struggle with fine-grained 3D scene understanding, while isolated sensor modalities fail to resolve ambiguities in long-tail driving scenarios. </p>
                 <p>Embodied intelligence further demands systems to "think" and "act" in a human-centric manner, integrating physical interaction, spatial reasoning, and iterative self-correction.   Recent breakthroughs, such as vision-tactile fusion systems for deformable object manipulation and transformer-based multimodal frameworks for driver-like scene comprehension, highlight the potential of combining multimodal perception with cognitive architectures.   Gaps persist in generalizing these capabilities to real-world deployment, particularly in balancing computational efficiency, ethical considerations, and safety-critical decision-making. </p>
                 <p>This workshop aims to bridge these gaps by fostering interdisciplinary discussions on multi-modal fusion techniques, embodied reasoning frameworks, and scalable learning paradigms for autonomous driving. </p>
                 <p class="mt-6 font-semibold text-gray-600">Keywords: autonomous driving, embodied intelligence, multimodal fusion. </p>
            </div>
        </section>

        <section id="schedule" class="my-12 scroll-mt-20">
            <h2 class="text-3xl section-title mb-6">Workshop Schedule</h2>
            <div class="max-w-4xl mx-auto">
                <div class="space-y-6">
                    <!-- Schedule Item 1 -->
                    <div class="p-4 rounded-lg border bg-gray-50">
                        <div class="flex flex-col md:flex-row">
                            <div class="w-full md:w-1/4 font-semibold text-gray-700 mb-2 md:mb-0">14:00 - 14:30 </div>
                            <div class="w-full md:w-3/4">
                                <h4 class="font-bold text-lg">Cognitive learning of Autonomous Vehicles in Open road traffic</h4>
                                <p class="text-gray-600">Speaker: Prof. Xiao Wang, Anhui University</p>
                                <div class="mt-3 pt-3 border-t border-gray-200 text-sm text-gray-600 space-y-2">
                                    <p><strong>Highlight:</strong> A cognitive learning framework enabling autonomous vehicles (AVs) to achieve human-like adaptability and safety in dynamic open-road environments is designed. By integrating (1) Spatial-Temporal Attention (STA) mechanisms to infer road-user intentions by dynamically prioritizing critical spatial regions and temporal segments; (2) Social Compliance Estimation via formalized "absolute right-of-way" (A_ROW) metrics and ROW-violation indices, ensuring socially acceptable interactions; (3) Deep Evolutionary Reinforcement Learning (DERL) combining Twin Delayed DDPG (TD3) with genetic algorithms to expand policy search space, avoid local optima, and balance safety, efficiency, and comfort. The framework demonstrates robust adaptation across traffic densities, bridging machine precision with human-like cognitive flexibility for safer, interpretable AV integration into real-world traffic.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- Schedule Item 2 -->
                    <div class="p-4 rounded-lg border bg-gray-50">
                        <div class="flex flex-col md:flex-row">
                            <div class="w-full md:w-1/4 font-semibold text-gray-700 mb-2 md:mb-0">14:30 - 15:00 </div>
                            <div class="w-full md:w-3/4">
                                <h4 class="font-bold text-lg">Beyond Discovery: An Identification-Aware Bayesian Optimization Approach And Its Applications in Transportation</h4>
                                <p class="text-gray-600">Speaker: Prof. Zhiyuan Liu, Southeast University</p>
                                <div class="mt-3 pt-3 border-t border-gray-200 text-sm text-gray-600 space-y-2">
                                    <p><strong>Highlight:</strong> In simulation-based transportation systems—such as autonomous driving in adverse weather—decision optimization under noisy environments remains a core challenge. This talk introduces an Identification-Aware Bayesian Optimization framework, which goes beyond merely finding high-performing solutions to robustly identifying them. By balancing exploration and exploitation with adaptive acquisition strategies, the proposed method enhances decision reliability and sample efficiency. Applications in traffic signal control and trajectory planning demonstrate how this approach supports embodied intelligence in uncertain, multimodal scenarios.</p>
                                </div>
                            </div>
                        </div>
                    </div>
                    <!-- Schedule Item 3 -->
                    <div class="p-4 rounded-lg border bg-gray-50">
                        <div class="flex flex-col md:flex-row">
                             <div class="w-full md:w-1/4 font-semibold text-gray-700 mb-2 md:mb-0">15:00 - 15:30  </div>
                             <div class="w-full md:w-3/4">
                                 <h4 class="font-bold text-lg">ECAFormer: Low-light Image Enhancement using Dual Cross Attention</h4>
                                 <p class="text-gray-600">Speaker: Dr. Weikai Li, Chongqing Jiaotong University</p>
                                 <div class="mt-3 pt-3 border-t border-gray-200 text-sm text-gray-600 space-y-2">
                                     <p><strong>Highlight:</strong> In real-world autonomous driving, vehicles must operate safely across a wide range of challenging visual conditions — from night-time urban roads to dim tunnels and rainy or foggy environments. Among these, low-light conditions are especially problematic, as they can severely compromise the performance of perception modules by obscuring critical visual cues, increasing noise, and reducing image contrast. Enhancing image quality in such scenarios is therefore essential for improving safety and reliability in autonomous systems. This presentation introduces ECAFormer, a lightweight and effective framework specifically designed for low-light image enhancement. By employing a novel dual cross-attention mechanism, ECAFormer enables mutual enhancement of semantic and visual features across multiple scales, effectively balancing global brightness correction and local detail preservation. Extensive experiments on benchmark datasets and real-world dark road scenes show that the model significantly improves visibility and robustness while maintaining computational efficiency.</p>
                                 </div>
                             </div>
                        </div>
                    </div>
                    <!-- Schedule Item 4 -->
                    <div class="p-4 rounded-lg border bg-gray-50">
                        <div class="flex flex-col md:flex-row">
                             <div class="w-full md:w-1/4 font-semibold text-gray-700 mb-2 md:mb-0">15:30 - 16:00  </div>
                             <div class="w-full md:w-3/4">
                                 <h4 class="font-bold text-lg">Multimodal Fusion Perception and Cognitive Computing</h4>
                                 <p class="text-gray-600">Speaker: Prof. Hui Zhang, Beijing Jiaotong University</p>
                                 <div class="mt-3 pt-3 border-t border-gray-200 text-sm text-gray-600 space-y-2">
                                     <p><strong>Highlight:</strong> In modern urban traffic systems, complex traffic scenarios pose significant challenges to traffic management and intelligent transportation systems. To effectively address these challenges, it is essential to leverage multi-view and multimodal information for collaborative perception. This presentation explores how information can be acquired from various perspectives by combining different sensors and data sources, such as cameras, LiDAR, and in-vehicle sensors. It also focuses on how diverse modalities, such as images and videos, can be jointly analyzed and interpreted. This presentation introduces attention-based methods for the fusion and processing of multi-modal information, enabling systems to integrate data more effectively. Furthermore, the presentation will discuss how multi-modal data can be transformed into higher-level semantic understanding of traffic scenes, including modeling of pedestrians, vehicles, and road conditions. Finally, it will examine key challenges and future directions for collaborative perception in intelligent transportation.</p>
                                 </div>
                             </div>
                        </div>
                    </div>
                    <!-- Schedule Item 5 -->
                    <div class="p-4 rounded-lg border bg-gray-50">
                        <div class="flex flex-col md:flex-row">
                             <div class="w-full md:w-1/4 font-semibold text-gray-700 mb-2 md:mb-0">16:00 - 16:30  </div>
                             <div class="w-full md:w-3/4">
                                 <h4 class="font-bold text-lg">Predicting Where Human Driver Should Look</h4>
                                 <p class="text-gray-600">Speaker: Prof. Zhixiong Nan, Chongqing University</p>
                                 <div class="mt-3 pt-3 border-t border-gray-200 text-sm text-gray-600 space-y-2">
                                     <p><strong>Highlight:</strong> According to the road traffic injuries report of WHO, nearly 1.3 million people die in traffic accidents every year, and a high proportion of death results from driver attention distraction. Therefore, predicting where human driver should look is extremely important for advanced assisted driving systems and applications. This presentation discusses how to accurately predict human driver attention by fusing multiple data sources, such as cameras and in-vehicle sensors. This presentation introduces a method that predict human driver attention by simulating human driving experience accumulation procedure. Furthermore, this presentation also introduces another method that integrates multi-fold top-down guidance with the bottom-up feature.</p>
                                 </div>
                             </div>
                        </div>
                    </div>
                    <!-- Panel Discussion -->
                    <div class="flex flex-col md:flex-row p-4 rounded-lg bg-blue-50 border-blue-200 border">
                         <div class="w-full md:w-1/4 font-bold text-blue-700 mb-2 md:mb-0">16:30 - 17:00  </div>
                         <div class="w-full md:w-3/4">
                             <h4 class="font-bold text-lg text-blue-800">Panel Discussion: Social and Economoic Impact of Embodied Intelligence for Autonomous Vehicles</h4>
                         </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="speakers" class="my-12 scroll-mt-20">
            <h2 class="text-3xl section-title mb-8">Speakers</h2>
            <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-10">
                <div class="speaker-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=XW" alt="Photo of Prof. Xiao Wang" class="w-24 h-24 rounded-full object-cover">
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Prof. Xiao Wang</h3>
                        <p class="text-gray-600">Anhui University</p>
                    </div>
                </div>
                <div class="speaker-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=ZL" alt="Photo of Prof. Zhiyuan Liu" class="w-24 h-24 rounded-full object-cover">
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Prof. Zhiyuan Liu</h3>
                        <p class="text-gray-600">Southeast University</p>
                    </div>
                </div>
                <div class="speaker-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=WL" alt="Photo of Dr. Weikai Li" class="w-24 h-24 rounded-full object-cover">
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Dr. Weikai Li</h3>
                        <p class="text-gray-600">Chongqing Jiaotong University</p>
                    </div>
                </div>
                 <div class="speaker-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=HZ" alt="Photo of Prof. Hui Zhang" class="w-24 h-24 rounded-full object-cover">
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Prof. Hui Zhang</h3>
                        <p class="text-gray-600">Beijing Jiaotong University</p>
                    </div>
                </div>
                 <div class="speaker-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=ZN" alt="Photo of Prof. Zhixiong Nan" class="w-24 h-24 rounded-full object-cover">
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Prof. Zhixiong Nan</h3>
                        <p class="text-gray-600">Chongqing University</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="organizers" class="my-12 scroll-mt-20">
            <h2 class="text-3xl section-title mb-8">Organizers</h2>
            <div class="grid md:grid-cols-2 lg:grid-cols-3 xl:grid-cols-5 gap-10">
                 <div class="organizer-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=HZ" alt="Photo of Hui Zhang" class="w-24 h-24 rounded-full object-cover">
                    <div>
                          <p class="font-semibold text-lg">Hui Zhang </p>
                          <p class="text-gray-500">Beijing Jiaotong University </p>
                    </div>
                </div>
                 <div class="organizer-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=WL" alt="Photo of Weikai Li" class="w-24 h-24 rounded-full object-cover">
                    <div>
                          <p class="font-semibold text-lg">Weikai Li </p>
                          <p class="text-gray-500">Chongqing Jiaotong University </p>
                    </div>
                </div>
                 <div class="organizer-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=ZN" alt="Photo of Zhixiong Nan" class="w-24 h-24 rounded-full object-cover">
                    <div>
                          <p class="font-semibold text-lg">Zhixiong Nan </p>
                          <p class="text-gray-500">Chongqing University </p>
                    </div>
                </div>
                 <div class="organizer-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=ZL" alt="Photo of Zhiyuan Liu" class="w-24 h-24 rounded-full object-cover">
                    <div>
                          <p class="font-semibold text-lg">Zhiyuan Liu </p>
                          <p class="text-gray-500">Southeast University </p>
                    </div>
                </div>
                 <div class="organizer-card flex items-center space-x-4">
                    <img src="https://placehold.co/100x100/e2e8f0/334155?text=XW" alt="Photo of Xiao Wang" class="w-24 h-24 rounded-full object-cover">
                    <div>
                          <p class="font-semibold text-lg">Xiao Wang </p>
                          <p class="text-gray-500">Anhui University </p>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="text-center py-6 mt-8 bg-gray-800 text-white">
        <p class="text-gray-400">&copy; 2025 ITSC Workshop. All rights reserved.</p>
    </footer>

    <script>
        // JavaScript for mobile menu toggle
        const mobileMenuButton = document.getElementById('mobile-menu-button');
        const mobileMenu = document.getElementById('mobile-menu');
        mobileMenuButton.addEventListener('click', () => {
            mobileMenu.classList.toggle('hidden');
        });

        // JavaScript for active navigation link highlighting on scroll
        const sections = document.querySelectorAll('section[id]');
        const navLinks = document.querySelectorAll('.nav-link');
        const homeLink = document.querySelector('.nav-link[href="#home"]');
        const header = document.querySelector('#home');

        const observerOptions = {
            root: null,
            rootMargin: '0px',
            threshold: 0.4 // A section is considered active when 40% is visible
        };

        const sectionObserver = new IntersectionObserver((entries) => {
            let activeSectionFound = false;
            entries.forEach(entry => {
                if (entry.isIntersecting && entry.intersectionRatio >= 0.4) {
                    const id = entry.target.getAttribute('id');
                    const navLink = document.querySelector(`.nav-link[href="#${id}"]`);
                    
                    navLinks.forEach(link => link.classList.remove('nav-link-active'));
                    if (navLink) {
                        navLink.classList.add('nav-link-active');
                    }
                    activeSectionFound = true;
                }
            });

        }, observerOptions);
        
        // Separate observer for the header to handle the "Home" link
        const headerObserver = new IntersectionObserver((entries) => {
             if (entries[0].isIntersecting) {
                 // Check if any other section is currently marked as active
                 const anyOtherActive = Array.from(navLinks).some(link => 
                     link !== homeLink && link.classList.contains('nav-link-active')
                 );
                 // Only highlight "Home" if no other section is considered active
                 if (!anyOtherActive) {
                     navLinks.forEach(link => link.classList.remove('nav-link-active'));
                     homeLink.classList.add('nav-link-active');
                 }
             }
        }, { threshold: 0.1 }); // Trigger when 10% of the header is visible

        // Observe all sections and the header
        sections.forEach(section => {
            sectionObserver.observe(section);
        });
        headerObserver.observe(header);

        // Initial check in case the page loads on a section
        window.addEventListener('load', () => {
            let foundActive = false;
            sections.forEach(section => {
                const rect = section.getBoundingClientRect();
                if (rect.top >= 0 && rect.top < window.innerHeight * 0.4) {
                      const id = section.getAttribute('id');
                      const navLink = document.querySelector(`.nav-link[href="#${id}"]`);
                      navLinks.forEach(link => link.classList.remove('nav-link-active'));
                      if(navLink) navLink.classList.add('nav-link-active');
                      foundActive = true;
                }
            });
            if (!foundActive) {
                homeLink.classList.add('nav-link-active');
            }
        });

    </script>
</body>
</html>
